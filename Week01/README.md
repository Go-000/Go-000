# 微服务概览与治理

## 微服务概览

### 单体架构

存在问题：

* 复杂度高，不易理解
* 开发速度慢
* 可拓展性低
* 部署周期长，容易出错

### 微服务架构

#### 概览

1. 特点
   1. 原子服务
   2. 独立进程
   3. 隔离部署
   4. 去中心化服务治理
2. 好处
   1. 小的服务代码少，易测试，易维护
   2. 单一职责
   3. 尽可能早的创建原型
   4. 可移植性高，更容易实验和采纳新的技术
3. 弊端
   1. 服务的拆分和定义是一种挑战
   2. 分布式系统带来的挑战
      * 消息传递过慢或者服务不可用
      * 更新不同服务的不同数据库
      * 分布式系统的测试
      * 应用升级可能波及多个服务
   3. 对运维的基础设施要求高

#### 组件服务化

多个微服务组合完成一个完整的用户场景。单一服务的局部变化只需重新部署对应的服务进程。

* kit：一个微服务的基础库
* service：业务代码+kit依赖+第三方依赖组成的业务微服务
* rpc+message queue：轻量级通讯

#### 按业务组织

[康威定律](https://zh.wikipedia.org/wiki/%E5%BA%B7%E5%A8%81%E5%AE%9A%E5%BE%8B)：设计系统的架构受制于产生这些设计的组织的沟通结构。

服务提供的能力和业务功能对应。比如订单服务和数据访问服务，前者反应了真实的订单业务，后者是一种技术抽象，不反应真实的业务。所以按照微服务架构理念划分业务时，是不应该存在数据访问服务的。

大前端(移动/web) =>网关接入=>业务服务=>平台服务=>基础设施(PaaS/SaaS)。

开发团队对软件在生产环境的运行负全部责任。

#### 去中心化

* 数据去中心化：每个服务都有自己的DB、缓存
* 治理去中心化：比如通过服务发现实现服务实例动态上下线，避免修改服务消费者的服务提供者列表；又比如直连账号服务，而不是通过nginx以防止流量中心化
* 技术去中心化：每个服务可以针对性的选择合适的技术解决方案，但是也要避免过度多元化，防止维护成本过高

#### 基础设施自动化

自动化包括测试和部署。单一进程拆分为多个进程之后，意味着开发、调试、测试、监控和部署的复杂度都会增大，必须要自动化基础设施来支持。

* CICD: Gitlab+Gitlab Hooks+k8s
* Testing: 测试环境、单元测试、API自动化测试
* 在线 运行时: k8s以及一系列Prometheus、ELK、Control Plane

#### 可用性&兼容性设计

1. Design for failure
   1. 隔离
   2. 超时控制
   3. 负载保护
   4. 限流
   5. 降级
   6. 重试
   7. 负载均衡
2. 服务提供者的变更可能会引发服务消费者的兼容性破坏，时刻谨记保持服务接口的兼容性
   1. 发送时要保守，接收时要开放：[斯博塔尔法则](https://en.wikipedia.org/wiki/Robustness_principle#:~:text=The%20principle%20is%20also%20known,what%20you%20accept%20from%20others.)

## 微服务设计

### API Gateway

#### 阶段一

请求处理模式：浏览器=>web server=>服务

问题：

1. 客户端到微服务直接通信，强耦合。客户端的版本难以收敛，导致服务重构及其困难。
2. 客户端多次请求，客户端聚合数据，工作量巨大，延迟高。**需要面向用户场景的API。**
3. 协议不利于统一，各个部门有差异，需要端来兼容。
4. 面向“端”的API适配，耦合到了内部服务。
5. 多终端兼容逻辑复杂，每个服务都需要处理。
6. 统一逻辑无法收敛，比如安全认证、限流。

#### 阶段二

请求处理模式：浏览器=>web server=>BFF=>服务

新增一个app-interface用于统一到协议出口，在服务内进行大量的dataset join，按照业务场景来设计粗粒度的API，给后续服务的演进带来很多优势：

* 轻量级交互：协议精简、聚合
* 差异服务：数据裁剪以及聚合、针对终端定制化API
* 动态升级：原有系统兼容升级，更新服务而非协议
* 沟通效率提升，协作模式演进为移动业务+网关小组

#### 阶段三

阶段二的问题：app-interface属于单点故障，严重代码缺陷或者流量洪峰可能引发集群宕机。同时单个模块也会导致后续业务集成复杂度高，团队之间沟通协调成本高，交付效率低下。所以需要将app-interface按照业务域拆分，这种模式称为API Composer。

参考：《微服务架构设计模式》中的使用API组合模式进行查询一节

#### 阶段四

阶段三的问题：很多跨切横面逻辑，比如安全认证、日志监控、限流熔断等。随着时间推移，代码变得越来越复杂。解决方式是跨横切面的功能，需要协调更新框架升级发版（路由、认证、限流、安全）全部上沉到API Gateway，而BFF层的开发人员可以更加专注于业务逻辑交付，实现了架构上的关注分离。除此之外，BFF还可以使用nodejs做服务端渲染(SSR)，而API Gateway上层还有CDN、4/7层负载均衡。常见API Gateway的实现有Envoy，Zuul，Kong，apisix等。

### 微服务的划分

服务边界的划分是微服务架构面临的第一个问题。实际项目中通常会采用两种不同的方式划分服务边界：

* 通过业务职能划分，例如客户服务部门提供客户服务功能，财务部门提供财务相关的职能。
* 通过DDD的限界上下文划分。限界上下文是DDD中用来划分不同业务边界的元素，这里的业务边界是指解决不同业务问题的问题域和对应的解决方案。

微服务的划分也促进了组织结构的演进，一个微服务由一个团队维护，形成一个闭环。另外需要注意微服务不要过细，过细对于使用者来说使用过于麻烦，并且性能上也不会太好。

### CQORS

应用程序分为命令端和查询端。命令端处理程序创建、更新和删除，查询端通过针对一个或多个物化视图执行查询来处理查询，这些物化视图通过订阅数据更改时发出的事件流而保持最新。

参考：《微服务架构设计模式》中的使用CQRS模式一节

#### 微服务的安全

对外网请求来说，通常在API Gateway进行统一的认证拦截。认证成功会使用JWT方式通过RPC元数据传递的方式带到BFF层，BFF校验Token完整性后注入到应用的Context中。BFF到下层的微服务建议是直接在RPC Request中带入用户身份信息请求服务。

对内部服务，一般要区分身份认证和授权。

* full trust
* half trust
* zero trust

## grpc&服务发现

### GRPC

* 多语言支持
* 轻量级、高性能：序列化支持PB和JSON
* 可插拔
* IDL：基于文件定义服务，可以通过proto3工具生成指定语言的数据结构、服务端端口和客户端stub
* 设计理念：支持自定义元数据传递
* 移动端：基于标准HTTP2设计，支持双向流、消息头压缩、单TCP的多路复用、服务端推送等特性使得grpc在移动端更省电和节省网络流量
* 服务非对象、消息而非引用：促进微服务的系统间粗粒度消息交互设计理念
* 负载无关：不同的服务需要使用不同的消息类型
* 流
* 阻塞式和非阻塞式：支持异步和同步处理在客户端和服务端间交互的消息序列
* 元数据交换：常见的横切面关注点，如认证或追踪，依赖数据交换
* 标准化状态码：客户端通常以有限方式响应API调用返回的错误

### 健康检查

主动健康检查可以在服务提供者服务不稳定时被消费者感知，临时从负载均衡中摘除，减少错误请求。服务提供者重新稳定后，重新加入消费者的负载均衡，恢复请求。

主动健康检查也可以被用于外挂方式的容器健康检查或者流量检测。

### 服务发现

#### 客户端发现

一个服务实例被启动时，它的网络地址会被写到注册表上；当实例终止时，再从注册表中删除。服务实例的注册表通过心跳机制动态刷新。客户端使用一个负载均衡算法，去选择一个可用的服务实例，来响应这个请求。

#### 服务端发现

客户端通过负载均衡向一个服务发送请求，这个负载均衡器会会查询服务注册表，并将请求路由到可用的服务实例上。服务实例在服务注册表上被注册和注销。

#### 对比

客户端发现使用直连的方式，比服务端发现少一次网络跳转。但是消费者需要内置特定的服务发现客户端和发现逻辑，并且各种语言需要有自己的实现。如果使用k8s，可以使用sidecar，其实相当于将服务端发现的负载均衡集成到本地。

服务端发现消费者无需关注发现细节，只需知道服务的DNS域名即可，支持异构语言开发，需要基础设施支撑，多了一次网络跳转，可能有性能损失。

微服务的核心是去中心化，我们使用客户端发现模式。

#### 选型

海量服务发现和注册，服务状态可以弱一致，需要的是AP系统，所以参考Eureka。试想两个场景，牺牲一致性，最终一致性的情况：

* 注册的事件延迟
* 注销的事件延迟

Eureka相关：

* 服务提供者
  * 有三类事件：Register, Renew, Cancel。
  * 服务提供者向一个eureka实例注册，这个实例会复制到其他eureka节点。注册之后30s一次发送心跳到eureka节点
  * 注册信息appid、地址以及元数据（权重、染色标签、集群等）
* 服务消费者启动时拉取实例，发起30s长轮询
* eureka实例
  * 在两个心跳周期之后没有收到心跳，会将服务从服务列表删除
  * 短时间里丢失了大量的心跳连接(15分钟内心跳低于期望值*85%)，开启自我保护，保留过期服务不删除
  * eureka挂掉期间不要发版，需要恢复之后等服务都重新注册
  * 使用时间戳作为版本号，可以防止老的数据刷掉新的数据

## 多集群&多租户

#### 多集群

L0服务(比如账号服务)多集群的必要性：

* 单一集群考虑，多节点保证可用性，通常使用N+2的 方式冗余部署
* 单一集群故障带来的负面影响考虑冗余多套集群
* 单个机房内的机房故障导致的问题

不同集群的服务启动后，从环境变量获取当下服务的集群，在服务发现注册的时候，带入这些元信息。不同的集群可以隔离使用不同的缓存资源。好处是多套冗余的集群对应多套独占的缓存，带来更好的性能和冗余能力，但是需要尽量避免业务隔离使用或者sharding带来的cache hit影响。为了解决cache hit的影响，可以退而求其次，让消费者连接全部集群，但是带来以下的问题：

* 长连接导致的内存和CPU开销，healthCheck可以高达30%
* 短连接极大的资源成本和延迟

解决方式是子集化，参考《SRE Google运维解密》利用划分子集限制连接池大小一节。

#### 多租户

多租户是指在一个微服务架构中允许多系统共存。租户可以是测试，金丝雀发布，影子系统，甚至是服务层或者产品线，使用租户能够保证代码的隔离性并且基于流量租户做路由决策。

染色发布：注册到服务发现的元数据加上特定的标签，服务请求带上对应的标签，使得处理请求是路由到特定的服务上。但是对于典型的基础组件，比如日志、指标、存储、消息队列、缓存以及配置等，基于租户信息隔离数据需要分别处理基础组件。

## 问题集

1. https://shimo.im/docs/x8dxHkQRcdCHX8j3/read
2. https://shimo.im/docs/WxJp66WCtjVwKDK3